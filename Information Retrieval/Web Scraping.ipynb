{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Web-Scrapping.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "27fe32baf07f35d2da024650d4ed0ade61fd3ea4",
        "id": "4WzUoIwoaOE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import conlltags2tree, tree2conlltags\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "import httplib2\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "93d78093e28de6124308f5ca136b10310203ad5c",
        "id": "1q3w751yaOE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#______________links in Page, code changes Start____________________# \n",
        "def get_links(url):\n",
        "    # Read the url\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(resp, from_encoding = resp.info().get_param('charset'))\n",
        "    links = []\n",
        "    for link in soup.find_all('a', href = True):\n",
        "        links.append(link['href'])\n",
        "    # Return all links present in that url\n",
        "    return links\n",
        "#______________links in Page, code changes End____________________# \n",
        "\n",
        "\n",
        "#_____________Get Meta Data, code changes Start______________________#\n",
        "def get_metadata(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text)\n",
        "    metas = soup.find_all('meta')\n",
        "    meta_data = [meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description']\n",
        "    meta_data = [w.rstrip() for w in meta_data]\n",
        "    meta_data = [w.replace('\\n',' ') for w in meta_data] \n",
        "    return meta_data\n",
        "#_______________Get Meta Data, code changes End_____________________#\n",
        "\n",
        "\n",
        "#_____________HTML Parsing, code changes Start____________________# \n",
        "def url_to_string(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    for script in soup([\"script\", \"style\", 'aside']):\n",
        "        script.extract()\n",
        "    data = \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
        "    return data\n",
        "#_____________HTML Parsing, code changes END____________________# \n",
        "\n",
        "\n",
        "#_____________Extracting nouns, noun forms in document, code changes Start__________#\n",
        "def extract_noun_forms(data): \n",
        "    nouns_list = []\n",
        "    sentences = sent_tokenize(data)\n",
        "    for sent in sentences:\n",
        "        #' '.join((e for e in sent if e.isalnum())\n",
        "        tokens = nltk.word_tokenize(sent)\n",
        "        tagged = nltk.pos_tag(tokens)   # Part-of-Speech Tagging\n",
        "        nouns = [word for word, pos in tagged if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
        "        nouns_list.append(nouns)\n",
        "    nouns_list = [x for sublist in nouns_list for x in sublist]\n",
        "    return nouns_list\n",
        "#_____________Extracting nouns, noun forms in document, code changes END__________#\n",
        "\n",
        "\n",
        "#_____________Entity and Relation Extraction, code changes Start____________________#\n",
        "# i-o-b tagging, places, organisations, Names, Landmarks tagging\n",
        "def get_continuous_chunks(text):\n",
        "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "    for i in chunked:\n",
        "        if type(i) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "    return continuous_chunk\n",
        "#_____________Entity and Relation Extraction, code changes End____________________#\n",
        "\n",
        "\n",
        "#_____________NAMED Entity Recognition and Extraction using Spacy, code changes Start____________________#\n",
        "def generate_ner_tags(doc):\n",
        "    out_list =[]\n",
        "    for word in doc:\n",
        "        # entities[word] = str(i.ent_iob_) + \"-\"+ str(i.ent_type_)\n",
        "        if word.ent_type_ in [\"ORG\", \"EVENT\", \"PERSON\", \"NORP\", \"GPE\", \"MONEY\", \"LOC\", \"WORK_OF_ART\"]:\n",
        "            my_list = [word, str(word.ent_iob_) + \"-\" + str(word.ent_type_)]\n",
        "            out_list.append(my_list)            \n",
        "    return out_list\n",
        "#_____________NAMED Entity Recognition and Extraction using Spacy, code changes End____________________#\n",
        "\n",
        "\n",
        "#________________Text Pre-processing, code changes Start_______________#\n",
        "stop_words = set(stopwords.words(\"english\"))  \n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "pattern = r'[^a-zA-z0-9\\s]' # include digits #r'[^a-zA-z\\s]' remove digits also\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Pre-processing: Sentence Splitting, Tokenization and Normalization \n",
        "def pre_process(text):\n",
        "    text = re.sub(pattern, '', text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_tokens = [lancaster.stem(word) for word in filtered_tokens]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#_______________Text Pre-processing, code changes End_____________#\n",
        "\n",
        "\n",
        "#______________Saving Files, code changes Start_____________#\n",
        "def save_text_file(filename, data):\n",
        "    with open(filename, 'w') as f:\n",
        "        for item in data:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "            \n",
        "def save_json(filename, data):\n",
        "    with open(filename, 'w') as fp:\n",
        "        json.dump(data, fp)\n",
        "        \n",
        "def save_html(file_name,html):\n",
        "    with open(file_name, 'w', encoding = 'utf-8') as f:\n",
        "         f.write(html)\n",
        "#_____________Saving Files, code changes End_____________#\n",
        "\n",
        "\n",
        "#____________TF-IDF Updated, code changes Start___________________#\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key = lambda x: (x[1], x[0]), reverse = True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn):\n",
        "    # get the feature names and tf-idf score of top n items\n",
        "    # use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        # keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "    # create a tuples of feature,score\n",
        "    # results = zip(feature_vals,score_vals)\n",
        "    results = {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    return results       \n",
        "\n",
        "def get_tf_idf(doc, corpus, num):\n",
        "    cv = CountVectorizer(stop_words = stop_words)\n",
        "    word_count_vector = cv.fit_transform(corpus)\n",
        "    \n",
        "    # TfidfTransformer to Compute Inverse Document Frequency (IDF)\n",
        "    tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
        "    tfidf_transformer.fit(word_count_vector)\n",
        "    \n",
        "    feature_names = cv.get_feature_names()\n",
        "    # generate tf-idf for the given document\n",
        "    tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
        "    # sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
        "    # extract only the top n\n",
        "    keywords = extract_topn_from_vector(feature_names, sorted_items, num)\n",
        "    return keywords\n",
        "#____________________TF-IDF Updated, code changes End_________________#\n",
        "\n",
        "\n",
        "#____________Web search using Cosine Similarity, code changes Start_______________#\n",
        "def web_search(text):\n",
        "    cv = CountVectorizer(stop_words = stop_words)\n",
        "    word_count_vector = cv.fit_transform(corpus)\n",
        "    tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
        "    tfidf_transformer.fit(word_count_vector)\n",
        "    full_x = tfidf_transformer.transform(cv.transform(corpus))\n",
        "    sample1 = tfidf_transformer.transform(cv.transform([pre_process(text)]))\n",
        "    cosine_similarities = linear_kernel(sample1,full_x).flatten()\n",
        "    # return the first link that closely matches the text entered and the corresponding links metadata \n",
        "    url = urls[cosine_similarities.argsort()[:-5:-1][0]] \n",
        "    return  url, get_metadata(url)\n",
        "#____________Web search using Cosine Similarity, Code changes End_______________#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bcf4a12c26b72f4b5c8cfc36639ef493fb9e6522",
        "id": "ngIJBNleaOE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url1 = \"https://csee.essex.ac.uk/staff/udo/index.html\"\n",
        "url2 = \"https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering\"\n",
        "\n",
        "urls = [url1, url2]\n",
        "\n",
        "data = []\n",
        "# HTML Parsing and saving all the text in data\n",
        "for i, url in enumerate(urls):\n",
        "    filename = \"html_parsed_doc_\" + str(i+1)\n",
        "    text = url_to_string(url)\n",
        "    save_text_file(filename, sent_tokenize(text))\n",
        "    data.append(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "52436c688d10550a023230bbe49a0828fc0e0d00",
        "id": "gL5nbAtqaOFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving meta data present in each link\n",
        "for i, dat in enumerate(urls):\n",
        "    filename = \"Meta_data_in_link_\" + str(i+1)\n",
        "    text = get_metadata(url)\n",
        "    save_text_file(filename, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cd62fea619eb723737b49af6db651e56902e51f9",
        "id": "Uj39wOhMaOFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving all the links present in each document\n",
        "for i, url in enumerate(urls):\n",
        "    filename = \"links_in_doc_\" + str(i+1)\n",
        "    text = get_links(url)\n",
        "    save_text_file(filename, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "472d8e6ce22b704a1cb5a692f1d43f084f25bbf2",
        "id": "PcDVV8j_aOFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving all the Named Entities present in each document\n",
        "for i, dat in enumerate(data):\n",
        "    filename = \"Named_Entities_in_doc_\" + str(i+1)\n",
        "    text = get_continuous_chunks(dat)\n",
        "    save_text_file(filename, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b0a9f0a724a4850160855273fedf196ca0fc27ef",
        "id": "iLdsmXUQaOFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NER recognition using spacy and saving importnat tags like PERSON, LOR, ORG, \n",
        "for i, dat in enumerate(data):\n",
        "    doc = nlp(data[i])\n",
        "    entities = generate_ner_tags(doc)\n",
        "    filename = \"NER_in_doc_\" + str(i+1)\n",
        "    save_text_file(filename, entities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0db092cd52c43c9a1325f33379ead9673242cf42",
        "id": "mkA7YTaKaOFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving all the Nouns and noun forms present in each document\n",
        "for i, dat in enumerate(data):\n",
        "    filename = \"Noun_and_noun_forms_in_doc_\" + str(i+1)\n",
        "    text = extract_noun_forms(dat)\n",
        "    save_text_file(filename, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2674d1373a287816e5452ce3aeac5cfd40fb804e",
        "id": "5V0O1F55aOFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting corpus for calculating tf-idf scores\n",
        "corpus = []\n",
        "for dat in data:\n",
        "    corpus.append(pre_process(dat))\n",
        "    \n",
        "# saving all the TF-IDF scores in each document\n",
        "for i, dat in enumerate(corpus):\n",
        "    keywords = get_tf_idf(dat, corpus, num = 20)\n",
        "    filename = \"top_tf_idf_scores_in_doc_\" + str(i+1) + \".json\"\n",
        "    save_json(filename, keywords)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "62d365400e5e461ac28cdb328934bdc7ca98f018",
        "id": "dcByR1QsaOFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = \"Udo\"\n",
        "web_search(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "79d4ccd5ffcf782c63ee436323008512a336b40a",
        "id": "hN2Pb78zaOFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = \"robotics\"\n",
        "web_search(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ab4648a04262f1cbf736af3096be9b7f4b804c88",
        "id": "7bvuHWm-aOFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving NER visually as html files to view whole data in page and corresponding tags\n",
        "for i, dat in enumerate(data):\n",
        "    doc = nlp(dat)\n",
        "    html = displacy.render(doc, style = 'ent', page = True)\n",
        "    filename = \"NER_Image_in_doc_\" + str(i+1) + \".html\"\n",
        "    save_html(filename, html)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}